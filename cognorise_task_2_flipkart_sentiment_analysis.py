# -*- coding: utf-8 -*-
"""CognoRise_Task:2_FlipKart Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UXepOv5IURE7mJu_TQ_56AYlPuRptsnL
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re

nltk.download('stopwords')
nltk.download('punkt')

#data_path='/content/drive/MyDrive/Dataset-FlipKart_Sentiment Analysis.csv'
df=pd.read_csv('/content/drive/MyDrive/Dataset-FlipKart_Sentiment Analysis.csv',encoding='ISO-8859-1')
df

df.head()

df.tail()

df.columns

df.drop(['product_name', 'product_price', 'Rate'],axis=1,inplace=True)

df.columns

df.isna().sum()

df['Summary'] = df['Summary'].fillna(df['Summary'].mode()[0])
df.isna().sum()

df.dtypes

df.info()

df.drop(['Review'],axis=1,inplace=True)

df.columns

#Rename the columns
df.rename(columns={'Summary':'Text'},inplace=True)

df['Sentiment'].unique()

df['Sentiment']=df['Sentiment'].map({'positive':1,'negative':-1,'neutral':0})

#Its an imbalanced data
sns.countplot(x=df['Sentiment'],data=df,edgecolor='k')

df.describe(include='all')

#Assign a variable for moview reviews
reviews =df.Text
reviews

#Tokenize the sentences
from nltk.tokenize import TweetTokenizer
tk=TweetTokenizer()
reviews=reviews.apply(lambda x:tk.tokenize(x)).apply(lambda x:' '.join(x))
#reviews

#Remove special characters
reviews=reviews.apply(lambda x:re.sub('[^a-zA-Z0-9 ]+', ' ', x))
#reviews

#Collect meaning ful words
reviews=reviews.apply(lambda x:[i for i in tk.tokenize(x) if len(i)>3]).apply(lambda x:' '.join(x))
#reviews

from nltk.stem import SnowballStemmer
st=SnowballStemmer('english')
reviews=reviews.apply(lambda x:[st.stem(i.lower()) for i in tk.tokenize(x)]).apply(lambda x:' '.join(x))
reviews

#Remove stop words
from nltk.corpus import stopwords
word = stopwords.words('english')
reviews = reviews.apply(lambda x:[i for i in tk.tokenize(x) if i not in word]).apply(lambda x:' '.join(x))
#reviews

#Vectorization
from sklearn.feature_extraction.text import TfidfVectorizer
vec=TfidfVectorizer()
data=vec.fit_transform(reviews)
print(data)

x=data
y=df['Sentiment'].values
x.shape

y

#Smote oversampling done to balance the output data classes
from imblearn.over_sampling import SMOTE
smote=SMOTE(random_state=42)
x_sm,y_sm=smote.fit_resample(x,y)
# Check the class distribution after oversampling
y_sm=pd.Series(y_sm)
print(y_sm.value_counts())

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x_sm,y_sm,test_size=0.20,random_state=42)
y_train[:20]

#Model creation
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,ConfusionMatrixDisplay
#model1=KNeighborsClassifier(n_neighbors=7)
model2=DecisionTreeClassifier(criterion='entropy')
model3=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=42)
#model4=SVC()
#model5=BernoulliNB()
lst=[model2,model3]

for i in lst:
  print("model is",i)
  i.fit(x_train,y_train)
  y_pred=i.predict(x_test)
  cm=confusion_matrix(y_test,y_pred)
  print("Accuracy score is",accuracy_score(y_test,y_pred))
  print(cm)
  labels=[1,-1,0]
  print(classification_report(y_test,y_pred))
  cmd=ConfusionMatrixDisplay(cm,display_labels=labels)
  cmd.plot()
  plt.show()